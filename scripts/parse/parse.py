import json
from datetime import datetime
from pathlib import Path

import click
import logging

from radar.make.gen_makefile import generate_makefile
from radar.parsers.config import config_parser, config_parser_cross_vm_covert
from radar.parsers.profiler import make_profiler_parser
from radar.parsers.result import make_result_parser
from radar.utils import chdir, chmkdir, save_json, make_mongodb, setup_logger


@click.group()
def parse():
    pass


@parse.command()
@click.option("-t", "--task_path", required=True, type=click.Path(exists=True))
def backup(task_path):
    """Backup the existing summary results, which is probably generated by
    old version of parser scripts"""
    with chdir(task_path):
        b = f"archived-{datetime.now().isoformat()}"
        s = Path("summary")
        if s.exists():
            s.rename(b)
        # Backup only results from old scripts that create 'summary' dir
        # New results should be overriden to utilize Makefile
        # s = Path("result")
        # if s.exists():
        #     s.rename(b)
        t = Path("task.yml")
        if t.exists():
            t.rename(f"{b}/task.yml")


@parse.command()
@click.option("-t", "--task_path", required=True, type=click.Path(exists=True))
@click.option(
    "-r", "--result_path", required=True, type=click.Path(), default="result"
)
@click.option('-c', '--cross_vm_covert', is_flag=True)
@click.option("-o", "--out_file", required=True, default="config.json")
def config(task_path, result_path, out_file, cross_vm_covert):
    task_path = Path(task_path).resolve()
    result_path = Path(result_path).resolve()

    if cross_vm_covert:
        parser = config_parser_cross_vm_covert(task_path)
    else:
        parser = config_parser(task_path)
    parser.parse()
    with chmkdir(result_path):
        parser.save(Path(out_file))


@parse.command()
@click.option("-t", "--task_path", required=True, type=click.Path(exists=True))
@click.option(
    "-r", "--result_path", required=True, type=click.Path(), default="result"
)
@click.option("-o", "--out_file", required=True, default="result.json")
def result(task_path, result_path, out_file):
    task_path = Path(task_path).resolve()
    result_path = Path(result_path).resolve()

    parser = make_result_parser(task_path, result_path)
    parser.parse()
    with chmkdir(result_path):
        parser.save(Path(out_file))


@parse.command()
@click.option("-t", "--task_path", required=True, type=click.Path(exists=True))
@click.option(
    "-r", "--result_path", required=True, type=click.Path(), default="result"
)
@click.option("-o", "--out_file", required=True, default="profiler.json")
def profiler(task_path, result_path, out_file):
    task_path = Path(task_path).resolve()
    result_path = Path(result_path).resolve()

    parser = make_profiler_parser(task_path, result_path)
    parser.parse()
    with chmkdir(result_path):
        parser.save(Path(out_file))


@parse.command()
@click.option(
    "-r", "--result_path", required=True, type=click.Path(), default="result"
)
@click.option("-o", "--out_file", required=True, default="summary.json")
def summary(result_path, out_file):
    result_path = Path(result_path).resolve()

    data = dict()
    with open(result_path / "config.json", "r") as f:
        data = json.load(f)

    with chmkdir(result_path):
        with open("result.json", "r") as fd:
            fdata = json.load(fd)
        if "result" not in data:
            data["result"] = fdata
        else:
            raise Exception(
                "Name conflict, should have 'result' field in config"
            )

        if Path("profiler.json").exists():
            with open("profiler.json", "r") as fd:
                fdata = json.load(fd)
            data["profiler"]["data"] = fdata

        # For task 13 only, refactor this code later
        if data["task"]["task"] == 13:
            for cfg in [
                "fence_freq",
                "fence_strategy",
                "flush_after_load",
                "flush_l1",
                "record_timing",
                "repeat",
            ]:
                res = data['result']['threads']['1']['iter_results'][0]
                if (cfg in res) and (cfg not in data['task']):
                    data['task'][cfg] = res[cfg]

        with open(Path(out_file), "w") as f:
            save_json(data, f)


@parse.command()
@click.option("-b", "--batch_path", required=True, type=click.Path())
@click.option('-c', '--cross_vm_covert', is_flag=True)
def gen_makefile(batch_path, cross_vm_covert):
    batch_path = Path(batch_path).resolve()
    parser_file = Path(__file__).resolve()
    generate_makefile(batch_path, parser_file, cross_vm_covert)


@parse.command()
@click.option(
    "-r",
    "--result_path",
    required=True,
    type=click.Path(exists=True),
    default="result",
)
@click.option("-s", "--summary_file", required=True, default="summary")
@click.option(
    "-d", "--dbcol_name", required=True, default="data:experiment_result"
)
@click.option("-o", "--out_file", required=True, default="update.log")
def update(result_path, summary_file, dbcol_name, out_file):
    result_path = Path(result_path).resolve()
    summary_file = (result_path / summary_file).resolve()

    with chdir(result_path):
        setup_logger(out_file)
        logger = logging.getLogger()

        with open(summary_file, "r") as f:
            summary = json.load(f)

        db = make_mongodb(dbcol_name)
        db.client.update_one(
            {"_id": summary["_id"]}, {"$set": summary}, upsert=True
        )
        logger.info(f"Mongodb update finished, _id={summary['_id']}")


if __name__ == "__main__":
    parse()
